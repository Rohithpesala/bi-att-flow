{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "\n",
    "from gensim import models\n",
    "from lib import graph, coarsening, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate domain specific word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(data_path, data_type):\n",
    "    data = json.load(open(data_path.format(data_type), 'r'))\n",
    "    sentences = []\n",
    "    for article in data['data']:\n",
    "        for para in article['paragraphs']:\n",
    "            context = para['context']\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "            for sentence in nltk.sent_tokenize(context):\n",
    "                sentences.append(nltk.word_tokenize(sentence))\n",
    "            for qa in para['qas']:\n",
    "                sentences.append(nltk.word_tokenize(qa['question']))\n",
    "    return sentences\n",
    "\n",
    "def generate_word_embeddings(data_path, datasets):\n",
    "    sentences = []\n",
    "    for dataset in datasets:\n",
    "        sentences += get_sentences(data_path, dataset)\n",
    "    w2v = models.Word2Vec(sentences)\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 32255.\n"
     ]
    }
   ],
   "source": [
    "squad_path = \"/Users/dthai/data/squad/{}-v1.1.json\"\n",
    "squad_w2v = generate_word_embeddings(squad_path, ['train', 'dev'])\n",
    "print(\"Vocab size {}.\".format(len(squad_w2v.wv.vocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 47538.\n"
     ]
    }
   ],
   "source": [
    "newsqa_path = \"/Users/dthai/data/newsqa/{}-v1.1.json\"\n",
    "newsqa_w2v = generate_word_embeddings(newsqa_path, ['train', 'dev', 'test'])\n",
    "print(\"Vocab size {}.\".format(len(newsqa_w2v.wv.vocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping vocab 21808.\n"
     ]
    }
   ],
   "source": [
    "overlap = set(squad_w2v.wv.vocab.keys()) & set(newsqa_w2v.wv.vocab.keys())\n",
    "print(\"Number of overlapping vocab {}.\".format(len(overlap)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_edges = 16\n",
    "coarsening_levels = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_graph(word_emb):\n",
    "    vocab = word_emb.wv.vocab\n",
    "    embeddings = np.empty((len(vocab), word_emb.vector_size))\n",
    "    for i, word in enumerate(vocab.keys()):\n",
    "        embeddings[i,:] = word_emb[word]\n",
    "\n",
    "    graph_data = embeddings\n",
    "    t_start = time.process_time()\n",
    "    dist, idx = graph.distance_sklearn_metrics(graph_data, k=number_edges, metric='cosine')\n",
    "    A = graph.adjacency(dist, idx)\n",
    "    print(\"{} > {} edges\".format(A.nnz//2, number_edges*graph_data.shape[0]//2))\n",
    "    A = graph.replace_random_edges(A, 0)\n",
    "    graphs, perm = coarsening.coarsen(A, levels=coarsening_levels, self_connections=False)\n",
    "    L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "    print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_data = embeddings\n",
    "t_start = time.process_time()\n",
    "dist, idx = graph.distance_sklearn_metrics(graph_data, k=number_edges, metric='cosine')\n",
    "A = graph.adjacency(dist, idx)\n",
    "print(\"{} > {} edges\".format(A.nnz//2, number_edges*graph_data.shape[0]//2))\n",
    "A = graph.replace_random_edges(A, 0)\n",
    "graphs, perm = coarsening.coarsen(A, levels=coarsening_levels, self_connections=False)\n",
    "L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "print('Execution time: {:.2f}s'.format(time.process_time() - t_start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
